{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical formula for water is H‚ÇÇO. This indicates that each molecule of water is composed of two hydrogen atoms (H) and one oxygen atom (O).\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "prompt = \"What is the chemical formula for water?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ],\n",
    "\n",
    "    # Optional parameters\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.1,\n",
    "    presence_penalty=0.1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeminiAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical formula of glucose is **C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import dotenv\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Set up the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "response = model.generate_content(\"What is the chemical formula of glucose?\")\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the food items in the fridge and their quantities, to the best of my ability to determine from the image:\n",
      "\n",
      "Top Shelf:\n",
      "\n",
      "* Avocados: 4-5 (partially obscured)\n",
      "* Green Onions: Large bunch\n",
      "* Mushrooms: 6-7\n",
      "* Fresh Herbs: 1 bunch\n",
      "* Salmon Fillet: 1 \n",
      "\n",
      "Bottom Shelf:\n",
      "\n",
      "* Salad Greens: 1 container\n",
      "* Edible Flowers: 6-8\n",
      "* Apples: 3\n",
      "* Oranges: 2\n",
      "* Lemon: 1\n",
      "* Kiwi Fruit: 2 \n",
      "\n",
      "Please note:  It's difficult to be exact with quantities for some items as they are partially hidden. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat with files\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "prompt_parts = [\n",
    "    genai.upload_file(\"fridge_food.jpg\"),\n",
    "    \"List the food items in the fridge and their quantities.\"\n",
    "]\n",
    "\n",
    "response = model.generate_content(prompt_parts)\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Les grands mod√®les de langage sont g√©niaux !\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat history\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "chat_history = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Hi!\"]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"Hi there! How can I help you today?\"],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"Translate 'Large Language Models are awesome!' to French.\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = model.generate_content(chat_history)\n",
    "\n",
    "try:\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Exception:\\n\", e, \"\\n\")\n",
    "    print(\"Response:\\n\", response.candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's awesome! Pizza is a classic for a reason. What's your favorite kind of pizza?  üçï \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat history auto\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\", generation_config={\"temperature\": 0.3})\n",
    "chat = model.start_chat(history=[])\n",
    "     \n",
    "prompt_parts = [\"My favourite food is pizza.\"]\n",
    "response = chat.send_message(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have access to your personal information, including your favorite food.  You told me your favorite food is pizza, though!  üòä  \n",
      "\n",
      "Is there anything else you'd like to tell me about your favorite food?  Maybe what kind of pizza you like best, or what you like about it? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_parts = [\"What is my favourite food?\"]\n",
    "response = chat.send_message(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"My favourite food is pizza.\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"That\\'s awesome! Pizza is a classic for a reason. What\\'s your favorite kind of pizza?  üçï \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"What is my favourite food?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"As an AI, I don\\'t have access to your personal information, including your favorite food.  You told me your favorite food is pizza, though!  üòä  \\n\\nIs there anything else you\\'d like to tell me about your favorite food?  Maybe what kind of pizza you like best, or what you like about it? \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic Claude API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest protein in the human body is generally considered to be thymosin beta-4, which consists of only 43 amino acids. This small protein plays several important roles in the body:\n",
      "\n",
      "1. Cell proliferation and differentiation: It helps regulate the growth and development of various cell types.\n",
      "\n",
      "2. Wound healing: Thymosin beta-4 promotes tissue repair and regeneration.\n",
      "\n",
      "3. Angiogenesis: It stimulates the formation of new blood vessels.\n",
      "\n",
      "4. Anti-inflammatory effects: The protein has been shown to have some anti-inflammatory properties.\n",
      "\n",
      "5. Actin regulation: It helps in the organization of the cellular cytoskeleton by binding to actin.\n",
      "\n",
      "While thymosin beta-4 is widely recognized as the shortest human protein, it's worth noting that there are some shorter peptides in the body. However, these are typically classified as peptide hormones rather than proteins. For example, the tripeptide thyrotropin-releasing hormone (TRH) consists of only three amino acids.\n",
      "\n",
      "It's also important to mention that ongoing research in proteomics and genomics may potentially identify even shorter functional proteins or peptides in the future.\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.3,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is the shortest protein in the human body?\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "---\n",
      "!\n",
      "---\n",
      " How\n",
      "---\n",
      " can\n",
      "---\n",
      " I\n",
      "---\n",
      " assist\n",
      "---\n",
      " you\n",
      "---\n",
      " today\n",
      "---\n",
      "?\n",
      "---\n",
      " Feel\n",
      "---\n",
      " free\n",
      "---\n",
      " to\n",
      "---\n",
      " ask\n",
      "---\n",
      " me\n",
      "---\n",
      " any\n",
      "---\n",
      " questions\n",
      "---\n",
      " or\n",
      "---\n",
      " let\n",
      "---\n",
      " me\n",
      "---\n",
      " know\n",
      "---\n",
      " if\n",
      "---\n",
      " you\n",
      "---\n",
      " nee\n",
      "---\n",
      "d help\n",
      "---\n",
      " with\n",
      "---\n",
      " something\n",
      "---\n",
      " specific\n",
      "---\n",
      ".\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "with client.messages.stream(\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    ") as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def file_to_base64(file):\n",
    "    return base64.b64encode(open(file, \"rb\").read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the image, this appears to be a Neapolitan-style pizza. Here are the likely ingredients and a basic method to recreate it:\n",
      "\n",
      "Ingredients:\n",
      "1. Pizza dough\n",
      "2. Tomato sauce\n",
      "3. Fresh mozzarella cheese\n",
      "4. Italian sausage or meatballs (crumbled)\n",
      "5. Fresh basil leaves\n",
      "6. Olive oil (not visible, but typically used)\n",
      "\n",
      "Instructions:\n",
      "1. Preheat your oven to the highest temperature (ideally 450-500¬∞F or higher) with a pizza stone or steel if you have one.\n",
      "\n",
      "2. Stretch out your pizza dough into a round shape on a floured surface.\n",
      "\n",
      "3. Spread a thin layer of tomato sauce over the dough, leaving a small border around the edges for the crust.\n",
      "\n",
      "4. Tear fresh mozzarella into pieces and distribute evenly over the sauce.\n",
      "\n",
      "5. Add small pieces of crumbled sausage or meatballs across the pizza.\n",
      "\n",
      "6. Drizzle with a little olive oil.\n",
      "\n",
      "7. Carefully transfer the pizza to the preheated oven (on the stone/steel if using) and bake for about 8-12 minutes, or until the crust is golden and the cheese is bubbly.\n",
      "\n",
      "8. Remove from the oven and immediately add fresh basil leaves.\n",
      "\n",
      "9. Let it cool for a minute or two, then slice and serve.\n",
      "\n",
      "The key to achieving the charred spots on the crust is high heat and a short cooking time, which is characteristic of Neapolitan-style pizzas.\n"
     ]
    }
   ],
   "source": [
    "image_media_type = \"image/jpeg\"\n",
    "image_data = file_to_base64(\"pizza.jpeg\")\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.3,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": image_media_type,\n",
    "                        \"data\": image_data,\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Tell me the ingredients and the instructions to make the pizza from the image.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
